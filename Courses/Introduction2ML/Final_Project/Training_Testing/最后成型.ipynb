{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-19T15:14:10.715637400Z",
     "start_time": "2024-01-19T15:14:10.700623300Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "#from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "\n",
    "    def __init__(self, penalty=\"l2\", gamma=0, fit_intercept=True):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        - penalty: str, \"l1\" or \"l2\". Determines the regularization to be used.\n",
    "        - gamma: float, regularization coefficient. Used in conjunction with 'penalty'.\n",
    "        - fit_intercept: bool, whether to add an intercept (bias) term.\n",
    "        \"\"\"\n",
    "        err_msg = \"penalty must be 'l1' or 'l2', but got: {}\".format(penalty)#汇报错误\n",
    "        assert penalty in [\"l2\", \"l1\"], err_msg\n",
    "        self.penalty = penalty\n",
    "        self.gamma = gamma\n",
    "        self.fit_intercept = fit_intercept#是否加入截距项\n",
    "        self.coef_ = None\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1/(np.exp(-x)+1)\n",
    "\n",
    "    def get_gradient(self, X, y, coef_):\n",
    "        return np.dot(X.T, (self.sigmoid(np.dot(X, coef_)) - y))\n",
    "\n",
    "\n",
    "    def fit(self, X, y, lr=0.01, tol=1e-7, max_iter=1e5,decay=0.75):#fit的意思是拟合参数，此处使用梯度下降法\n",
    "        '''\n",
    "        :param X:\n",
    "        :param y:\n",
    "        :param lr:\n",
    "        :param tol:\n",
    "        :param max_iter:\n",
    "        :return losses:\n",
    "        '''\n",
    "\n",
    "        if self.fit_intercept:\n",
    "            X_tilde = np.c_[np.ones(X.shape[0]), X]  # c_是按列连接两个矩阵，np.ones(X.shape[0])是一个全1的矩阵，X是原矩阵\n",
    "        else:\n",
    "            X_tilde = X\n",
    "        # Initialize coefficients\n",
    "        self.coef_ = np.zeros(X_tilde.shape[1])  # coef_是系数矩阵，初始化为全0矩阵\n",
    "        \n",
    "        # List to store loss values at each iteration\n",
    "        losses = []\n",
    "        y_pred=self.sigmoid(np.dot(X_tilde,self.coef_))#\n",
    "\n",
    "        for i in range(int(max_iter)):\n",
    "\n",
    "            loss=-y*np.dot(X_tilde, self.coef_)+np.log(1+np.exp(np.dot(X_tilde,self.coef_)))\n",
    "            loss=loss.sum()\n",
    "            losses.append(loss)\n",
    "\n",
    "            if self.penalty=='l2':\n",
    "                self.coef_ = self.coef_ - lr * (self.get_gradient(X_tilde, y, self.coef_)+self.gamma*self.coef_)\n",
    "            else:\n",
    "                self.coef_ = self.coef_ - lr * (self.get_gradient(X_tilde, y, self.coef_)+self.gamma*np.sign(self.coef_))\n",
    "            y_pred = self.sigmoid(np.dot(X_tilde, self.coef_))\n",
    "\n",
    "            print(f'    iteration:{i},    loss:{loss:.2e}')\n",
    "            if i>1 and losses[-2]-losses[-1]<0:\n",
    "                lr=lr*decay\n",
    "            if lr<tol:\n",
    "                break\n",
    "\n",
    "        return losses\n",
    "\n",
    "    def predict(self, X):#在已经训练好模型后进行预测，此处使用sigmoid函数\n",
    "        \"\"\"\n",
    "        Use the trained model to generate prediction probabilities on a new\n",
    "        collection of data points.\n",
    "        \n",
    "        Parameters:\n",
    "        - X: numpy array of shape (n_samples, n_features), input data.\n",
    "        \n",
    "        Returns:\n",
    "        - probs: numpy array of shape (n_samples,), prediction probabilities.\n",
    "        \"\"\"\n",
    "        if self.fit_intercept:\n",
    "            X_tilde = np.c_[np.ones(X.shape[0]), X]\n",
    "\n",
    "        # Compute the linear combination of inputs and weights\n",
    "        linear_output = np.dot(X_tilde, self.coef_)\n",
    "\n",
    "        return np.where(self.sigmoid(linear_output)>=0.5,1,0)\n",
    "\n",
    "    def cal_accuracy(self,y_pred_test,y_test):\n",
    "        y_pred=y_pred_test\n",
    "        #返回一个百分数，并保留4位小数，需要带百分号\n",
    "        return f'accuracy:{100*np.mean(y_pred==y_test):.4f}%'\n",
    "    def cal_f1_score(self,y_pred_test,y_test):\n",
    "        y_pred=y_pred_test\n",
    "        TP=np.sum((y_pred==1)&(y_test==1))\n",
    "        FP=np.sum((y_pred==1)&(y_test==0))\n",
    "        FN=np.sum((y_pred==0)&(y_test==1))\n",
    "        precision=TP/(TP+FP)\n",
    "        recall=TP/(TP+FN)\n",
    "        return f'f1_score:{2*precision*recall/(precision+recall):.4f}'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T15:14:10.763215600Z",
     "start_time": "2024-01-19T15:14:10.716625700Z"
    }
   },
   "id": "b40b50e0641720e6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 为测试集做准备"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e2e759977388df3"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "label=pd.read_csv('labels.csv')\n",
    "# 读取数据\n",
    "df = pd.read_excel('training_dataset.xls')\n",
    "# 划分数据\n",
    "split_ratio = 0\n",
    "split_idx = int(label.shape[0] * split_ratio)\n",
    "X_test = df.iloc[split_idx:, :].reset_index(drop=True)\n",
    "y_test = label.iloc[split_idx:, :].reset_index(drop=True).iloc[:,1]\n",
    "# 保存测试数据集\n",
    "with pd.ExcelWriter('testing_dataset.xlsx') as writer:\n",
    "    X_test.to_excel(writer, sheet_name='X_test', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T15:14:36.395392Z",
     "start_time": "2024-01-19T15:14:10.725188100Z"
    }
   },
   "id": "82544428b14d77fd"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "class PB21000000():\n",
    "    def __init__(self):\n",
    "        with open('model_params.json', 'r') as file:\n",
    "            self.model_params = json.load(file)\n",
    "        \n",
    "        # 使用加载的参数初始化模型\n",
    "        self.model = LogisticRegression(\n",
    "            penalty=self.model_params['penalty'],\n",
    "            gamma=self.model_params['gamma'],\n",
    "            fit_intercept=self.model_params['fit_intercept']\n",
    "        )\n",
    "        self.model.coef_ = np.array(self.model_params['coef'])\n",
    "        self.MEAN = self.model_params['MEAN']\n",
    "        self.STD = self.model_params['STD']\n",
    "        self.Mean_4_fillna = pd.Series(self.model_params['Mean_4_fillna']) if 'Mean_4_fillna' in self.model_params else None\n",
    "        self.Prob_4_fillna = {key: pd.Series(value) for key, value in self.model_params['Prob_4_fillna'].items()} if 'Prob_4_fillna' in self.model_params else None\n",
    "        self.delete_list = self.model_params['delete_list']\n",
    "        \n",
    "    def testingset_data_processing(self,data_path):\n",
    "        df=pd.read_excel(data_path)\n",
    "        # 处理时间戳\n",
    "        df['Time Stamp']=df['Time Stamp'][:].apply(lambda x:x[:6]+'20'+x[8:])\n",
    "        # 处理缺失值\n",
    "        for i in range(len(self.delete_list)):\n",
    "            df.drop(self.delete_list[i],axis=1,inplace=True)\n",
    "        df_labels=df[['WW','W2']].copy()\n",
    "        df_labels['WW'].fillna('无',inplace=True)\n",
    "        df_labels['W2'].replace('阵性','阵雨',inplace=True)\n",
    "        df_labels['W2'].replace('雷暴，有降水或无','雷暴，有降雨或无',inplace=True)\n",
    "        df_labels['W2'].fillna('无',inplace=True)\n",
    "        # 字里行间看出来的label\n",
    "        df_labels['WW'].astype('str')\n",
    "        df_labels['W2'].astype('str')\n",
    "        df['WW']=np.array([df_labels['WW'][i].find('雨')>0 for i in range(df.shape[0])])\n",
    "        df['W2']=np.array([df_labels['W2'][i].find('雨')>0 for i in range(df.shape[0])])\n",
    "        df['VV'].replace('低于 0.1',0,inplace=True) # 目的是把低于0.1替换成0，以减少onhot编码的维度\n",
    "        # 取保时间戳是datetime格式\n",
    "        df['Time Stamp'] = pd.to_datetime(df['Time Stamp'], format='%d.%m.%Y %H:%M')\n",
    "        \n",
    "        # 缺失值处理的第二步\n",
    "        # 处理缺失值，若是float或int类型，用均值或者中位数填充；若是str，用多项分布进行随机填充\n",
    "        for col in df.columns:\n",
    "            if df[col].dtype=='float64' or df[col].dtype=='int64':\n",
    "                mean=self.Mean_4_fillna[col]\n",
    "                df[col].fillna(mean,inplace=True)\n",
    "                # df[col].fillna(df[col].median(),inplace=True) #用中位数填充\n",
    "            elif df[col].dtype=='object':\n",
    "                \n",
    "                prob=self.Prob_4_fillna[col]\n",
    "                #df[col]=df[col].apply(lambda x:prob.index[np.random.multinomial(1,prob).argmax()] if pd.isnull(x) else x)\n",
    "                # 用众数填充\n",
    "                df[col].fillna(df[col].mode()[0],inplace=True)\n",
    "        # onehot编码\n",
    "        df_timestamp=df['Time Stamp'].copy()\n",
    "        df.drop(['Time Stamp'],axis=1,inplace=True)\n",
    "        df.drop(['RRR'],axis=1,inplace=True)\n",
    "        # 为onehot编码做准备\n",
    "        df_onehot = pd.get_dummies(df,dtype='float64')\n",
    "        \n",
    "        # 合并数据\n",
    "        df_onehot['Time Stamp']=df_timestamp\n",
    "        df_onehot=df_onehot[['Time Stamp']+list(df_onehot.columns[:-1])]\n",
    "        \n",
    "        # 合并小时数据成为一天的数据\n",
    "        #Time Stamp里面是小时的数据，这里按照天取平均\n",
    "        df_onehot['Time Stamp']=pd.to_datetime(df_onehot['Time Stamp'],format='%Y-%m-%d %H:%M:%S')\n",
    "        # 对每天的数据进行处理，取平均\n",
    "        for col in df_onehot.columns[1:]:\n",
    "            df_onehot_X=df_onehot.groupby(df_onehot['Time Stamp'].dt.date)[col].mean()\n",
    "            df_onehot[col]=df_onehot['Time Stamp'].dt.date.map(df_onehot_X)\n",
    "        return (df_onehot.iloc[:,1:]-self.MEAN)/self.STD\n",
    "        \n",
    "        \n",
    "    def predict(self, data_path): \n",
    "        df_onehot = self.testingset_data_processing(data_path)\n",
    "        return self.model.predict(df_onehot)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T15:14:36.412473100Z",
     "start_time": "2024-01-19T15:14:36.407393700Z"
    }
   },
   "id": "e68f3ec8d2ccf7de"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_accuracy:0.93\n",
      "f1_score:f1_score:0.9185\n"
     ]
    }
   ],
   "source": [
    "CLASS = PB21000000()\n",
    "y_test_pred=CLASS.predict('testing_dataset.xlsx')\n",
    "print(f'test_accuracy:{np.sum(y_test_pred==y_test)/len(y_test):.2f}')\n",
    "print(f'f1_score:{CLASS.model.cal_f1_score(y_test_pred,y_test)}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T15:14:58.247784900Z",
     "start_time": "2024-01-19T15:14:36.414512700Z"
    }
   },
   "id": "1aae0bb12c32fd5c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
