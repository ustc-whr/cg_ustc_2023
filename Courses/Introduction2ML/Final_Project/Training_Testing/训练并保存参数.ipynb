{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-19T15:40:40.974208400Z",
     "start_time": "2024-01-19T15:40:40.958331800Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "\n",
    "    def __init__(self, penalty=\"l2\", gamma=0, fit_intercept=True):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        - penalty: str, \"l1\" or \"l2\". Determines the regularization to be used.\n",
    "        - gamma: float, regularization coefficient. Used in conjunction with 'penalty'.\n",
    "        - fit_intercept: bool, whether to add an intercept (bias) term.\n",
    "        \"\"\"\n",
    "        err_msg = \"penalty must be 'l1' or 'l2', but got: {}\".format(penalty)#汇报错误\n",
    "        assert penalty in [\"l2\", \"l1\"], err_msg\n",
    "        self.penalty = penalty\n",
    "        self.gamma = gamma\n",
    "        self.fit_intercept = fit_intercept#是否加入截距项\n",
    "        self.coef_ = None\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1/(np.exp(-x)+1)\n",
    "\n",
    "    def get_gradient(self, X, y, coef_):\n",
    "        return np.dot(X.T, (self.sigmoid(np.dot(X, coef_)) - y))\n",
    "\n",
    "\n",
    "    def fit(self, X, y, lr=0.01, tol=1e-7, max_iter=1e5,decay=0.75):#fit的意思是拟合参数，此处使用梯度下降法\n",
    "        '''\n",
    "        :param X:\n",
    "        :param y:\n",
    "        :param lr:\n",
    "        :param tol:\n",
    "        :param max_iter:\n",
    "        :return losses:\n",
    "        '''\n",
    "\n",
    "        if self.fit_intercept:\n",
    "            X_tilde = np.c_[np.ones(X.shape[0]), X]  # c_是按列连接两个矩阵，np.ones(X.shape[0])是一个全1的矩阵，X是原矩阵\n",
    "        else:\n",
    "            X_tilde = X\n",
    "        # Initialize coefficients\n",
    "        self.coef_ = np.zeros(X_tilde.shape[1])  # coef_是系数矩阵，初始化为全0矩阵\n",
    "        \n",
    "        # List to store loss values at each iteration\n",
    "        losses = []\n",
    "        y_pred=self.sigmoid(np.dot(X_tilde,self.coef_))#\n",
    "\n",
    "        for i in range(int(max_iter)):\n",
    "\n",
    "            loss=-y*np.dot(X_tilde, self.coef_)+np.log(1+np.exp(np.dot(X_tilde,self.coef_)))\n",
    "            loss=loss.sum()\n",
    "            losses.append(loss)\n",
    "\n",
    "            if self.penalty=='l2':\n",
    "                self.coef_ = self.coef_ - lr * (self.get_gradient(X_tilde, y, self.coef_)+self.gamma*self.coef_)\n",
    "            else:\n",
    "                self.coef_ = self.coef_ - lr * (self.get_gradient(X_tilde, y, self.coef_)+self.gamma*np.sign(self.coef_))\n",
    "            y_pred = self.sigmoid(np.dot(X_tilde, self.coef_))\n",
    "\n",
    "            print(f'    iteration:{i},    loss:{loss:.2e}')\n",
    "            if i>1 and losses[-2]-losses[-1]<0:\n",
    "                lr=lr*decay\n",
    "            if lr<tol:\n",
    "                break\n",
    "\n",
    "        return losses\n",
    "\n",
    "    def predict(self, X):#在已经训练好模型后进行预测，此处使用sigmoid函数\n",
    "        \"\"\"\n",
    "        Use the trained model to generate prediction probabilities on a new\n",
    "        collection of data points.\n",
    "        \n",
    "        Parameters:\n",
    "        - X: numpy array of shape (n_samples, n_features), input data.\n",
    "        \n",
    "        Returns:\n",
    "        - probs: numpy array of shape (n_samples,), prediction probabilities.\n",
    "        \"\"\"\n",
    "        if self.fit_intercept:\n",
    "            X_tilde = np.c_[np.ones(X.shape[0]), X]\n",
    "\n",
    "        # Compute the linear combination of inputs and weights\n",
    "        linear_output = np.dot(X_tilde, self.coef_)\n",
    "\n",
    "        return np.where(self.sigmoid(linear_output)>=0.5,1,0)\n",
    "\n",
    "    def cal_accuracy(self,y_pred_test,y_test):\n",
    "        y_pred=y_pred_test\n",
    "        #返回一个百分数，并保留4位小数，需要带百分号\n",
    "        return f'accuracy:{100*np.mean(y_pred==y_test):.4f}%'\n",
    "    def cal_f1_score(self,y_pred_test,y_test):\n",
    "        y_pred=y_pred_test\n",
    "        TP=np.sum((y_pred==1)&(y_test==1))\n",
    "        FP=np.sum((y_pred==1)&(y_test==0))\n",
    "        FN=np.sum((y_pred==0)&(y_test==1))\n",
    "        precision=TP/(TP+FP)\n",
    "        recall=TP/(TP+FN)\n",
    "        return f'f1_score:{2*precision*recall/(precision+recall):.4f}'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T15:40:40.984525600Z",
     "start_time": "2024-01-19T15:40:40.974208400Z"
    }
   },
   "id": "b40b50e0641720e6"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def trainingset_data_processing(path):\n",
    "    df=pd.read_excel(path)\n",
    "    # 处理时间戳\n",
    "    df['Time Stamp']=df['Time Stamp'][:].apply(lambda x:x[:6]+'20'+x[8:])\n",
    "    # 处理缺失值\n",
    "    df_null_rate=df.isnull().sum()/len(df)\n",
    "    delete_list=[]\n",
    "    # 删除缺失值超过80%的列，并记录下来 （缺失值处理的第一步）\n",
    "    for i in range(1,df.shape[1]):\n",
    "        if df_null_rate[i]>0.8:\n",
    "            delete_list.append(df.columns[i])\n",
    "    delete_list.append('Nh')\n",
    "    delete_list.append('H')\n",
    "    for i in range(len(delete_list)):\n",
    "        df.drop(delete_list[i],axis=1,inplace=True)\n",
    "    # 处理labels\n",
    "    df_labels=df[['RRR','WW','W2']].copy()\n",
    "    df_labels['WW'].fillna('无',inplace=True)\n",
    "    df_labels['W2'].replace('阵性','阵雨',inplace=True)\n",
    "    df_labels['W2'].replace('雷暴，有降水或无','雷暴，有降雨或无',inplace=True)\n",
    "    df_labels['W2'].fillna('无',inplace=True)\n",
    "    # 处理RRR(是原始数据上的label,但是有缺失值)\n",
    "    df_labels['RRR'].fillna(False,inplace=True)#inplace=True表示在原数据上修改,原数据是df_labels\n",
    "    df_labels['RRR'].replace('无降水',False,inplace=True)\n",
    "    df_labels['RRR'].replace('降水迹象',True,inplace=True)\n",
    "    df_labels['RRR'] = df_labels['RRR'].apply(lambda x:True if (isinstance(x, float) or (isinstance(x, int) and not isinstance(x, bool))) else x)\n",
    "    # 字里行间看出来的label\n",
    "    df_labels['WW'].astype('str')\n",
    "    df_labels['W2'].astype('str')\n",
    "    df['WW']=np.array([df_labels['WW'][i].find('雨')>0 for i in range(df.shape[0])])\n",
    "    df['W2']=np.array([df_labels['W2'][i].find('雨')>0 for i in range(df.shape[0])])\n",
    "    df_labels['part_label']=np.array([df_labels['WW'][i].find('雨')>0 or df_labels['W2'][i].find('雨')>0  for i in range(df_labels.shape[0])])\n",
    "    # 合并label\n",
    "    df_labels['real_label']=df_labels['RRR']|df_labels['part_label']\n",
    "    df_labels['RRR_raw']=df['RRR'].copy()\n",
    "    #调整位置，把RRR_raw放在第一列\n",
    "    df_labels=df_labels[['RRR_raw','RRR','WW','W2','part_label','real_label']]\n",
    "    # 取保时间戳是datetime格式\n",
    "    df['Time Stamp'] = pd.to_datetime(df['Time Stamp'], format='%d.%m.%Y %H:%M')\n",
    "    # 存入可以被训练的labels\n",
    "    df['real_label']=df_labels['real_label'].copy()\n",
    "    # labels转换为0和1，为后续Logistic Regression 做准备\n",
    "    df['real_label'].replace(True,1,inplace=True)\n",
    "    df['real_label'].replace(False,0,inplace=True)\n",
    "    df['VV'].replace('低于 0.1',0,inplace=True) # 目的是把低于0.1替换成0，以减少onhot编码的维度\n",
    "    # 缺失值处理的第二步\n",
    "    # 处理缺失值，若是float或int类型，用均值或者中位数填充；若是str，用多项分布进行随机填充\n",
    "    # 同时要记录下prob，用于测试集的填充\n",
    "    Mean_4_fillna={}\n",
    "    Prob_4_fillna={}\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype=='float64' or df[col].dtype=='int64':\n",
    "            Mean_4_fillna[col]=df[col].mean()\n",
    "            df[col].fillna(df[col].mean(),inplace=True)\n",
    "            # df[col].fillna(df[col].median(),inplace=True) #用中位数填充\n",
    "        elif df[col].dtype=='object':\n",
    "            # 用众数填充\n",
    "            df[col].fillna(df[col].mode()[0],inplace=True)\n",
    "            # 记录众数\n",
    "            Prob_4_fillna[col]=df[col].mode()[0]\n",
    "            #prob=df[col].value_counts(normalize=True)\n",
    "            #Prob_4_fillna[col]=prob\n",
    "            #df[col]=df[col].apply(lambda x:prob.index[np.random.multinomial(1,prob).argmax()] if pd.isnull(x) else x)\n",
    "    # onehot编码\n",
    "    df_timestamp=df['Time Stamp'].copy()\n",
    "    df.drop(['Time Stamp'],axis=1,inplace=True)\n",
    "    LABELS=df['real_label'].copy()\n",
    "    df.drop(['real_label'],axis=1,inplace=True)\n",
    "    df_RRR=df['RRR'].copy()\n",
    "    df.drop(['RRR'],axis=1,inplace=True)\n",
    "    # 为onehot编码做准备\n",
    "    df_RRR.replace('降水迹象',df_RRR[df_RRR.apply(lambda x:isinstance(x,float))].mean(),inplace=True)\n",
    "    df_RRR.replace('无降水',0,inplace=True)\n",
    "    \n",
    "    # onehot编码,并记录下来，为后续测试集的onehot编码做准备\n",
    "    df_onehot = pd.get_dummies(df,dtype='float64')\n",
    "    df_dummies=df_onehot\n",
    "    \n",
    "    # 合并数据\n",
    "    df_onehot['Time Stamp']=df_timestamp\n",
    "    df_onehot['real_label']=LABELS\n",
    "    df_onehot=df_onehot[['Time Stamp']+list(df_onehot.columns[:-2])+['real_label']]\n",
    "    \n",
    "    # 合并小时数据成为一天的数据\n",
    "    #Time Stamp里面是小时的数据，这里按照天取平均\n",
    "    df_onehot['Time Stamp']=pd.to_datetime(df_onehot['Time Stamp'],format='%Y-%m-%d %H:%M:%S')\n",
    "    # 对每天的label进行处理，取最大值\n",
    "    for col in df_onehot.columns[1:-1]:\n",
    "        df_onehot_X=df_onehot.groupby(df_onehot['Time Stamp'].dt.date)[col].mean()\n",
    "        df_onehot[col]=df_onehot['Time Stamp'].dt.date.map(df_onehot_X)\n",
    "    df_onehot_y=df_onehot.groupby(df_onehot['Time Stamp'].dt.date)['real_label'].max()\n",
    "    df_onehot['real_label']=df_onehot['Time Stamp'].dt.date.map(df_onehot_y)\n",
    "    return df_onehot,Mean_4_fillna,Prob_4_fillna,delete_list,df_dummies"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T15:40:41.012604800Z",
     "start_time": "2024-01-19T15:40:40.994536600Z"
    }
   },
   "id": "77e673ab8a995d11"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def split_train_test(df_onehot):\n",
    "    split_rate=1\n",
    "    #划分训练集和测试集\n",
    "    X_train,X_test,y_train,y_test=df_onehot.iloc[:int(df_onehot.shape[0]*split_rate),1:-1],df_onehot.iloc[int(df_onehot.shape[0]*split_rate):,1:-1],df_onehot.iloc[:int(df_onehot.shape[0]*split_rate),-1],df_onehot.iloc[int(df_onehot.shape[0]*split_rate):,-1]\n",
    "    # 标准化\n",
    "    MEAN=X_train.mean()\n",
    "    STD=X_train.std()\n",
    "    X_train=(X_train-MEAN)/STD\n",
    "    X_test=(X_test-MEAN)/STD\n",
    "    # 标准化,并记录scaler，用于测试集的标准化\n",
    "    #scaler=StandardScaler()\n",
    "    #X_train=scaler.fit_transform(X_train)\n",
    "    #X_test=scaler.transform(X_test)\n",
    "    return X_train,X_test,y_train,y_test,MEAN,STD"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T15:40:41.018607600Z",
     "start_time": "2024-01-19T15:40:41.005826800Z"
    }
   },
   "id": "fb47e487b20eb63e"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "df_onehot,Mean_4_fillna,Prob_4_fillna,delete_list,df_dummies=trainingset_data_processing('training_dataset.xls')\n",
    "pd.DataFrame(df_onehot.iloc[:,-1]).to_csv('labels.csv')\n",
    "X_train,X_test,y_train,y_test,MEAN,STD=split_train_test(df_onehot)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T15:40:46.885141400Z",
     "start_time": "2024-01-19T15:40:41.018607600Z"
    }
   },
   "id": "1638ba94336194d"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    iteration:0,    loss:2.83e+04\n",
      "    iteration:1,    loss:1.10e+05\n",
      "    iteration:2,    loss:7.89e+04\n",
      "    iteration:3,    loss:8.58e+04\n",
      "    iteration:4,    loss:1.59e+05\n",
      "    iteration:5,    loss:1.19e+05\n",
      "    iteration:6,    loss:5.37e+04\n",
      "    iteration:7,    loss:3.80e+04\n",
      "    iteration:8,    loss:3.15e+04\n",
      "    iteration:9,    loss:2.65e+04\n",
      "    iteration:10,    loss:2.24e+04\n",
      "    iteration:11,    loss:1.90e+04\n",
      "    iteration:12,    loss:1.65e+04\n",
      "    iteration:13,    loss:1.55e+04\n",
      "    iteration:14,    loss:1.99e+04\n",
      "    iteration:15,    loss:4.71e+04\n",
      "    iteration:16,    loss:4.66e+04\n",
      "    iteration:17,    loss:2.28e+04\n",
      "    iteration:18,    loss:1.61e+04\n",
      "    iteration:19,    loss:1.33e+04\n",
      "    iteration:20,    loss:1.19e+04\n",
      "    iteration:21,    loss:1.09e+04\n",
      "    iteration:22,    loss:1.01e+04\n",
      "    iteration:23,    loss:9.53e+03\n",
      "    iteration:24,    loss:9.05e+03\n",
      "    iteration:25,    loss:8.66e+03\n",
      "    iteration:26,    loss:8.33e+03\n",
      "    iteration:27,    loss:8.06e+03\n",
      "    iteration:28,    loss:7.83e+03\n",
      "    iteration:29,    loss:7.65e+03\n",
      "    iteration:30,    loss:7.53e+03\n",
      "    iteration:31,    loss:7.63e+03\n",
      "    iteration:32,    loss:8.91e+03\n",
      "    iteration:33,    loss:1.07e+04\n",
      "    iteration:34,    loss:9.69e+03\n",
      "    iteration:35,    loss:7.42e+03\n",
      "    iteration:36,    loss:7.22e+03\n",
      "    iteration:37,    loss:7.13e+03\n",
      "    iteration:38,    loss:7.07e+03\n",
      "    iteration:39,    loss:7.03e+03\n",
      "    iteration:40,    loss:6.99e+03\n",
      "    iteration:41,    loss:6.96e+03\n",
      "    iteration:42,    loss:6.93e+03\n",
      "    iteration:43,    loss:6.90e+03\n",
      "    iteration:44,    loss:6.88e+03\n",
      "    iteration:45,    loss:6.86e+03\n",
      "    iteration:46,    loss:6.85e+03\n",
      "    iteration:47,    loss:6.83e+03\n",
      "    iteration:48,    loss:6.82e+03\n",
      "    iteration:49,    loss:6.81e+03\n",
      "    iteration:50,    loss:6.80e+03\n",
      "    iteration:51,    loss:6.79e+03\n",
      "    iteration:52,    loss:6.78e+03\n",
      "    iteration:53,    loss:6.77e+03\n",
      "    iteration:54,    loss:6.76e+03\n",
      "    iteration:55,    loss:6.76e+03\n",
      "    iteration:56,    loss:6.75e+03\n",
      "    iteration:57,    loss:6.75e+03\n",
      "    iteration:58,    loss:6.74e+03\n",
      "    iteration:59,    loss:6.74e+03\n",
      "    iteration:60,    loss:6.73e+03\n",
      "    iteration:61,    loss:6.73e+03\n",
      "    iteration:62,    loss:6.73e+03\n",
      "    iteration:63,    loss:6.73e+03\n",
      "    iteration:64,    loss:6.72e+03\n",
      "    iteration:65,    loss:6.72e+03\n",
      "    iteration:66,    loss:6.72e+03\n",
      "    iteration:67,    loss:6.72e+03\n",
      "    iteration:68,    loss:6.72e+03\n",
      "    iteration:69,    loss:6.72e+03\n",
      "    iteration:70,    loss:6.71e+03\n",
      "    iteration:71,    loss:6.71e+03\n",
      "    iteration:72,    loss:6.71e+03\n",
      "    iteration:73,    loss:6.71e+03\n",
      "    iteration:74,    loss:6.71e+03\n",
      "    iteration:75,    loss:6.71e+03\n",
      "    iteration:76,    loss:6.71e+03\n",
      "    iteration:77,    loss:6.71e+03\n",
      "    iteration:78,    loss:6.71e+03\n",
      "    iteration:79,    loss:6.71e+03\n",
      "    iteration:80,    loss:6.71e+03\n",
      "    iteration:81,    loss:6.70e+03\n",
      "    iteration:82,    loss:6.70e+03\n",
      "    iteration:83,    loss:6.70e+03\n",
      "    iteration:84,    loss:6.70e+03\n",
      "    iteration:85,    loss:6.70e+03\n",
      "    iteration:86,    loss:6.70e+03\n",
      "    iteration:87,    loss:6.70e+03\n",
      "    iteration:88,    loss:6.70e+03\n",
      "    iteration:89,    loss:6.70e+03\n",
      "    iteration:90,    loss:6.70e+03\n",
      "    iteration:91,    loss:6.70e+03\n",
      "    iteration:92,    loss:6.70e+03\n",
      "    iteration:93,    loss:6.70e+03\n",
      "    iteration:94,    loss:6.70e+03\n",
      "    iteration:95,    loss:6.70e+03\n",
      "    iteration:96,    loss:6.70e+03\n",
      "    iteration:97,    loss:6.70e+03\n",
      "    iteration:98,    loss:6.70e+03\n",
      "    iteration:99,    loss:6.70e+03\n",
      "    iteration:100,    loss:6.70e+03\n",
      "    iteration:101,    loss:6.70e+03\n",
      "    iteration:102,    loss:6.70e+03\n",
      "    iteration:103,    loss:6.70e+03\n",
      "    iteration:104,    loss:6.70e+03\n",
      "    iteration:105,    loss:6.70e+03\n",
      "    iteration:106,    loss:6.70e+03\n",
      "    iteration:107,    loss:6.70e+03\n",
      "    iteration:108,    loss:6.70e+03\n",
      "    iteration:109,    loss:6.70e+03\n",
      "    iteration:110,    loss:6.70e+03\n",
      "    iteration:111,    loss:6.70e+03\n",
      "    iteration:112,    loss:6.70e+03\n",
      "    iteration:113,    loss:6.69e+03\n",
      "    iteration:114,    loss:6.69e+03\n",
      "    iteration:115,    loss:6.69e+03\n",
      "    iteration:116,    loss:6.69e+03\n",
      "    iteration:117,    loss:6.69e+03\n",
      "    iteration:118,    loss:6.69e+03\n",
      "    iteration:119,    loss:6.69e+03\n",
      "    iteration:120,    loss:6.69e+03\n",
      "    iteration:121,    loss:6.69e+03\n",
      "    iteration:122,    loss:6.69e+03\n",
      "    iteration:123,    loss:6.69e+03\n",
      "    iteration:124,    loss:6.69e+03\n",
      "    iteration:125,    loss:6.69e+03\n",
      "    iteration:126,    loss:6.69e+03\n",
      "    iteration:127,    loss:6.69e+03\n",
      "    iteration:128,    loss:6.69e+03\n",
      "    iteration:129,    loss:6.69e+03\n",
      "    iteration:130,    loss:6.69e+03\n",
      "    iteration:131,    loss:6.69e+03\n",
      "    iteration:132,    loss:6.69e+03\n",
      "    iteration:133,    loss:6.69e+03\n",
      "    iteration:134,    loss:6.69e+03\n",
      "    iteration:135,    loss:6.69e+03\n",
      "    iteration:136,    loss:6.69e+03\n",
      "    iteration:137,    loss:6.69e+03\n",
      "    iteration:138,    loss:6.69e+03\n",
      "    iteration:139,    loss:6.69e+03\n",
      "    iteration:140,    loss:6.69e+03\n",
      "    iteration:141,    loss:6.69e+03\n",
      "    iteration:142,    loss:6.69e+03\n",
      "    iteration:143,    loss:6.69e+03\n",
      "    iteration:144,    loss:6.69e+03\n",
      "    iteration:145,    loss:6.69e+03\n",
      "    iteration:146,    loss:6.69e+03\n",
      "    iteration:147,    loss:6.69e+03\n",
      "    iteration:148,    loss:6.69e+03\n",
      "    iteration:149,    loss:6.69e+03\n",
      "    iteration:150,    loss:6.69e+03\n",
      "    iteration:151,    loss:6.69e+03\n",
      "    iteration:152,    loss:6.69e+03\n",
      "    iteration:153,    loss:6.69e+03\n",
      "    iteration:154,    loss:6.69e+03\n",
      "    iteration:155,    loss:6.69e+03\n",
      "    iteration:156,    loss:6.69e+03\n",
      "    iteration:157,    loss:6.69e+03\n",
      "    iteration:158,    loss:6.69e+03\n",
      "    iteration:159,    loss:6.69e+03\n",
      "    iteration:160,    loss:6.69e+03\n",
      "    iteration:161,    loss:6.69e+03\n",
      "    iteration:162,    loss:6.69e+03\n",
      "    iteration:163,    loss:6.69e+03\n",
      "    iteration:164,    loss:6.69e+03\n",
      "    iteration:165,    loss:6.69e+03\n",
      "    iteration:166,    loss:6.69e+03\n",
      "    iteration:167,    loss:6.69e+03\n",
      "    iteration:168,    loss:6.69e+03\n",
      "    iteration:169,    loss:6.69e+03\n",
      "    iteration:170,    loss:6.69e+03\n",
      "    iteration:171,    loss:6.69e+03\n",
      "    iteration:172,    loss:6.69e+03\n",
      "    iteration:173,    loss:6.69e+03\n",
      "    iteration:174,    loss:6.69e+03\n",
      "    iteration:175,    loss:6.69e+03\n",
      "    iteration:176,    loss:6.69e+03\n",
      "    iteration:177,    loss:6.69e+03\n",
      "    iteration:178,    loss:6.69e+03\n",
      "    iteration:179,    loss:6.69e+03\n",
      "    iteration:180,    loss:6.69e+03\n",
      "    iteration:181,    loss:6.69e+03\n",
      "    iteration:182,    loss:6.69e+03\n",
      "    iteration:183,    loss:6.69e+03\n",
      "    iteration:184,    loss:6.69e+03\n",
      "    iteration:185,    loss:6.69e+03\n",
      "    iteration:186,    loss:6.69e+03\n",
      "    iteration:187,    loss:6.69e+03\n",
      "    iteration:188,    loss:6.69e+03\n",
      "    iteration:189,    loss:6.69e+03\n",
      "    iteration:190,    loss:6.69e+03\n",
      "    iteration:191,    loss:6.69e+03\n",
      "    iteration:192,    loss:6.69e+03\n",
      "    iteration:193,    loss:6.69e+03\n",
      "    iteration:194,    loss:6.69e+03\n",
      "    iteration:195,    loss:6.69e+03\n",
      "    iteration:196,    loss:6.69e+03\n",
      "    iteration:197,    loss:6.69e+03\n",
      "    iteration:198,    loss:6.69e+03\n",
      "    iteration:199,    loss:6.69e+03\n",
      "    iteration:200,    loss:6.69e+03\n",
      "    iteration:201,    loss:6.69e+03\n",
      "    iteration:202,    loss:6.69e+03\n",
      "    iteration:203,    loss:6.69e+03\n",
      "    iteration:204,    loss:6.69e+03\n",
      "    iteration:205,    loss:6.69e+03\n",
      "    iteration:206,    loss:6.69e+03\n",
      "    iteration:207,    loss:6.69e+03\n",
      "    iteration:208,    loss:6.69e+03\n",
      "    iteration:209,    loss:6.69e+03\n",
      "    iteration:210,    loss:6.69e+03\n",
      "    iteration:211,    loss:6.69e+03\n",
      "    iteration:212,    loss:6.69e+03\n",
      "    iteration:213,    loss:6.69e+03\n",
      "    iteration:214,    loss:6.69e+03\n",
      "    iteration:215,    loss:6.69e+03\n",
      "    iteration:216,    loss:6.69e+03\n",
      "    iteration:217,    loss:6.69e+03\n",
      "    iteration:218,    loss:6.69e+03\n",
      "    iteration:219,    loss:6.69e+03\n",
      "    iteration:220,    loss:6.69e+03\n",
      "    iteration:221,    loss:6.69e+03\n",
      "    iteration:222,    loss:6.69e+03\n",
      "    iteration:223,    loss:6.69e+03\n",
      "    iteration:224,    loss:6.69e+03\n",
      "    iteration:225,    loss:6.69e+03\n",
      "    iteration:226,    loss:6.69e+03\n",
      "    iteration:227,    loss:6.69e+03\n",
      "    iteration:228,    loss:6.69e+03\n",
      "    iteration:229,    loss:6.69e+03\n",
      "    iteration:230,    loss:6.69e+03\n",
      "    iteration:231,    loss:6.69e+03\n",
      "    iteration:232,    loss:6.69e+03\n",
      "    iteration:233,    loss:6.69e+03\n",
      "    iteration:234,    loss:6.69e+03\n",
      "    iteration:235,    loss:6.69e+03\n",
      "    iteration:236,    loss:6.69e+03\n",
      "    iteration:237,    loss:6.69e+03\n",
      "    iteration:238,    loss:6.69e+03\n",
      "    iteration:239,    loss:6.69e+03\n",
      "    iteration:240,    loss:6.69e+03\n",
      "    iteration:241,    loss:6.69e+03\n",
      "    iteration:242,    loss:6.69e+03\n",
      "    iteration:243,    loss:6.69e+03\n",
      "    iteration:244,    loss:6.69e+03\n",
      "    iteration:245,    loss:6.69e+03\n",
      "    iteration:246,    loss:6.69e+03\n",
      "    iteration:247,    loss:6.69e+03\n",
      "    iteration:248,    loss:6.69e+03\n",
      "    iteration:249,    loss:6.69e+03\n",
      "    iteration:250,    loss:6.69e+03\n",
      "    iteration:251,    loss:6.69e+03\n",
      "    iteration:252,    loss:6.69e+03\n",
      "    iteration:253,    loss:6.69e+03\n",
      "    iteration:254,    loss:6.69e+03\n",
      "    iteration:255,    loss:6.69e+03\n",
      "    iteration:256,    loss:6.69e+03\n",
      "    iteration:257,    loss:6.69e+03\n",
      "    iteration:258,    loss:6.69e+03\n",
      "    iteration:259,    loss:6.69e+03\n",
      "    iteration:260,    loss:6.69e+03\n",
      "    iteration:261,    loss:6.69e+03\n",
      "    iteration:262,    loss:6.69e+03\n",
      "    iteration:263,    loss:6.69e+03\n",
      "    iteration:264,    loss:6.69e+03\n",
      "    iteration:265,    loss:6.69e+03\n",
      "    iteration:266,    loss:6.69e+03\n",
      "    iteration:267,    loss:6.69e+03\n",
      "    iteration:268,    loss:6.69e+03\n",
      "    iteration:269,    loss:6.69e+03\n",
      "    iteration:270,    loss:6.69e+03\n",
      "    iteration:271,    loss:6.69e+03\n",
      "    iteration:272,    loss:6.69e+03\n",
      "    iteration:273,    loss:6.69e+03\n",
      "    iteration:274,    loss:6.69e+03\n",
      "    iteration:275,    loss:6.69e+03\n",
      "    iteration:276,    loss:6.69e+03\n",
      "    iteration:277,    loss:6.69e+03\n",
      "    iteration:278,    loss:6.69e+03\n",
      "    iteration:279,    loss:6.69e+03\n",
      "    iteration:280,    loss:6.69e+03\n",
      "    iteration:281,    loss:6.69e+03\n",
      "    iteration:282,    loss:6.69e+03\n",
      "    iteration:283,    loss:6.69e+03\n",
      "    iteration:284,    loss:6.69e+03\n",
      "    iteration:285,    loss:6.69e+03\n",
      "    iteration:286,    loss:6.69e+03\n",
      "    iteration:287,    loss:6.69e+03\n",
      "    iteration:288,    loss:6.69e+03\n",
      "    iteration:289,    loss:6.69e+03\n",
      "    iteration:290,    loss:6.69e+03\n",
      "    iteration:291,    loss:6.69e+03\n",
      "    iteration:292,    loss:6.69e+03\n",
      "    iteration:293,    loss:6.69e+03\n",
      "    iteration:294,    loss:6.69e+03\n",
      "    iteration:295,    loss:6.69e+03\n",
      "    iteration:296,    loss:6.69e+03\n",
      "    iteration:297,    loss:6.69e+03\n",
      "    iteration:298,    loss:6.69e+03\n",
      "    iteration:299,    loss:6.69e+03\n"
     ]
    },
    {
     "data": {
      "text/plain": "[28262.38314015121,\n 110420.72815514854,\n 78915.4738254297,\n 85848.41047552445,\n 158894.70623305687,\n 118947.6757566019,\n 53731.67261251242,\n 38030.587688860855,\n 31513.27966279298,\n 26483.568847230876,\n 22364.265829843935,\n 19012.246615107902,\n 16513.69713103341,\n 15482.20045825149,\n 19938.616566908007,\n 47055.47495389013,\n 46644.99907002318,\n 22761.59176058242,\n 16130.377420223344,\n 13255.223182737012,\n 11902.531000723453,\n 10901.50526752747,\n 10129.02633387796,\n 9526.540363093292,\n 9047.186590811236,\n 8656.083921847743,\n 8331.257525771656,\n 8059.439023884854,\n 7832.39786478707,\n 7648.4168315716815,\n 7529.638666427059,\n 7630.7316990158,\n 8905.361912147288,\n 10694.145397413155,\n 9687.433536836255,\n 7418.963782271492,\n 7223.360237281322,\n 7133.989689480433,\n 7072.770084913591,\n 7025.620161290734,\n 6987.368245862915,\n 6955.315980514968,\n 6927.8248134838395,\n 6903.844587175918,\n 6882.682415640126,\n 6863.8668805730595,\n 6847.0641376156,\n 6832.025848209924,\n 6818.557118886923,\n 6806.496984918294,\n 6795.706688357858,\n 6786.062753522061,\n 6777.45298931589,\n 6769.77425368737,\n 6762.9312559623795,\n 6756.83594451456,\n 6751.407197905888,\n 6746.570645714788,\n 6742.2585158256725,\n 6738.409451371392,\n 6734.968271373674,\n 6731.885669075991,\n 6729.117854247197,\n 6726.626152422212,\n 6724.37657679376,\n 6722.339388522169,\n 6720.488659609884,\n 6718.801849965777,\n 6717.259407437354,\n 6715.84439680522,\n 6714.542161252505,\n 6713.340017762338,\n 6712.226986293917,\n 6711.193551421506,\n 6710.231454337811,\n 6709.333512656699,\n 6708.493465230561,\n 6707.705839160639,\n 6706.965836268968,\n 6706.269236473204,\n 6705.61231572581,\n 6704.991776420727,\n 6704.4046884160325,\n 6703.848439057423,\n 6703.320690807863,\n 6702.81934528868,\n 6702.3425127156925,\n 6701.888485870172,\n 6701.45571787997,\n 6701.04280320228,\n 6700.648461298523,\n 6700.27152257546,\n 6699.910916237044,\n 6699.565659750522,\n 6699.234849679477,\n 6698.917653677512,\n 6698.613303470339,\n 6698.321088682264,\n 6698.040351386529,\n 6697.7704812783295,\n 6697.510911385489,\n 6697.2611142450105,\n 6697.0205984849235,\n 6696.788905759902,\n 6696.5656079969485,\n 6696.350304913624,\n 6696.142621776884,\n 6695.942207374804,\n 6695.748732177451,\n 6695.561886666166,\n 6695.381379813313,\n 6695.206937696759,\n 6695.038302235321,\n 6694.875230033046,\n 6694.717491321646,\n 6694.5648689916015,\n 6694.4171577035295,\n 6694.274163072307,\n 6694.13570091727,\n 6694.001596572503,\n 6693.871684251817,\n 6693.7458064635985,\n 6693.623813471166,\n 6693.5055627947095,\n 6693.390918751225,\n 6693.2797520292725,\n 6693.171939295575,\n 6693.067362830858,\n 6692.96591019249,\n 6692.867473901713,\n 6692.771951153513,\n 6692.679243547236,\n 6692.5892568363415,\n 6692.501900695748,\n 6692.4170885053645,\n 6692.334737148569,\n 6692.254766824429,\n 6692.177100872639,\n 6692.101665610153,\n 6692.0283901786415,\n 6691.957206401952,\n 6691.8880486528,\n 6691.820853728001,\n 6691.755560731614,\n 6691.6921109654095,\n 6691.630447826106,\n 6691.570516708901,\n 6691.512264916827,\n 6691.455641575511,\n 6691.400597552962,\n 6691.347085384012,\n 6691.2950591990975,\n 6691.244474657069,\n 6691.195288881752,\n 6691.147460402003,\n 6691.100949095013,\n 6691.055716132648,\n 6691.011723930624,\n 6690.968936100314,\n 6690.927317403036,\n 6690.886833706635,\n 6690.847451944231,\n 6690.809140074984,\n 6690.771867046757,\n 6690.73560276054,\n 6690.700318036561,\n 6690.66598458193,\n 6690.632574959784,\n 6690.600062559782,\n 6690.568421569919,\n 6690.537626949562,\n 6690.507654403626,\n 6690.478480357851,\n 6690.450081935105,\n 6690.422436932649,\n 6690.3955238003255,\n 6690.36932161961,\n 6690.343810083496,\n 6690.318969477139,\n 6690.294780659265,\n 6690.271225044256,\n 6690.248284584917,\n 6690.225941755876,\n 6690.204179537579,\n 6690.182981400863,\n 6690.162331292083,\n 6690.1422136187475,\n 6690.12261323567,\n 6690.1035154315705,\n 6690.084905916163,\n 6690.0667708076535,\n 6690.049096620663,\n 6690.031870254545,\n 6690.015078982098,\n 6689.998710438633,\n 6689.982752611384,\n 6689.9671938292795,\n 6689.952022753021,\n 6689.937228365474,\n 6689.922799962365,\n 6689.908727143273,\n 6689.894999802869,\n 6689.881608122471,\n 6689.868542561804,\n 6689.8557938510585,\n 6689.843352983147,\n 6689.83121120622,\n 6689.8193600163895,\n 6689.807791150681,\n 6689.796496580179,\n 6689.785468503393,\n 6689.774699339796,\n 6689.76418172357,\n 6689.753908497525,\n 6689.743872707201,\n 6689.734067595127,\n 6689.72448659526,\n 6689.715123327578,\n 6689.705971592832,\n 6689.697025367441,\n 6689.688278798535,\n 6689.6797261991505,\n 6689.671362043544,\n 6689.663180962653,\n 6689.655177739681,\n 6689.647347305807,\n 6689.639684736018,\n 6689.632185245059,\n 6689.624844183493,\n 6689.617657033877,\n 6689.610619407046,\n 6689.603727038502,\n 6689.596975784895,\n 6689.590361620619,\n 6689.583880634485,\n 6689.577529026505,\n 6689.571303104758,\n 6689.565199282337,\n 6689.559214074396,\n 6689.553344095268,\n 6689.5475860556735,\n 6689.5419367599925,\n 6689.536393103632,\n 6689.530952070447,\n 6689.52561073025,\n 6689.520366236391,\n 6689.51521582338,\n 6689.510156804607,\n 6689.505186570115,\n 6689.500302584423,\n 6689.495502384428,\n 6689.490783577356,\n 6689.486143838771,\n 6689.481580910642,\n 6689.477092599462,\n 6689.472676774421,\n 6689.468331365636,\n 6689.46405436241,\n 6689.45984381157,\n 6689.455697815829,\n 6689.451614532197,\n 6689.447592170446,\n 6689.443628991611,\n 6689.439723306539,\n 6689.435873474461,\n 6689.4320779016325,\n 6689.428335039987,\n 6689.4246433858425,\n 6689.421001478638,\n 6689.417407899706,\n 6689.41386127108,\n 6689.410360254336,\n 6689.406903549474,\n 6689.403489893804,\n 6689.40011806091,\n 6689.396786859591,\n 6689.393495132873,\n 6689.390241757024,\n 6689.387025640614,\n 6689.383845723587,\n 6689.380700976361,\n 6689.377590398972,\n 6689.374513020215,\n 6689.371467896829,\n 6689.368454112695,\n 6689.36547077806,\n 6689.362517028788,\n 6689.35959202562,\n 6689.35669495347,\n 6689.353825020726,\n 6689.35098145858,\n 6689.348163520381,\n 6689.345370480987,\n 6689.342601636166,\n 6689.339856301984,\n 6689.337133814223,\n 6689.334433527827,\n 6689.33175481634,\n 6689.329097071378,\n 6689.326459702111]"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=LogisticRegression()\n",
    "model.fit(X_train,y_train,lr=0.001,max_iter=300,decay=0.75)\n",
    "#y_pred=model.predict(X_test)\n",
    "#print(f'test_accuracy:{np.sum(y_pred==y_test)/len(y_test)},f1:{np.sum(y_pred*y_test)/(np.sum(y_pred)+np.sum(y_test)-np.sum(y_pred*y_test))}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T15:40:48.741794300Z",
     "start_time": "2024-01-19T15:40:46.887142400Z"
    }
   },
   "id": "8df565ee0eb73ff8"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# 假设`model`是您训练好的LogisticRegression模型实例\n",
    "model_params = {\n",
    "    'coef': model.coef_.tolist(),  # 将numpy数组转换为列表以进行JSON序列化\n",
    "    'penalty': model.penalty,\n",
    "    'gamma': model.gamma,\n",
    "    'fit_intercept': model.fit_intercept,\n",
    "    'MEAN': MEAN.tolist(),\n",
    "    'STD': STD.tolist(),\n",
    "    'Mean_4_fillna': Mean_4_fillna.tolist() if isinstance(Mean_4_fillna, pd.Series) else Mean_4_fillna,\n",
    "    'Prob_4_fillna' : {key: value for key, value in Prob_4_fillna.items()} if isinstance(Prob_4_fillna, dict) else Prob_4_fillna,\n",
    "    'delete_list': delete_list\n",
    "}\n",
    "\n",
    "# 保存为JSON\n",
    "with open('model_params.json', 'w') as file:\n",
    "    json.dump(model_params, file)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T15:40:48.741794300Z",
     "start_time": "2024-01-19T15:40:48.729280400Z"
    }
   },
   "id": "24b31f3800792788"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
